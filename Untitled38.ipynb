{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled38.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNW5j2gbhaLjvVFoRIqwfcF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tlkt/mofan_work/blob/master/Untitled38.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63_6GVpgM6SE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as Data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import math\n",
        "import zipfile\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrOWxXueNSf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = \"\"\n",
        "with zipfile.ZipFile('jaychou_lyrics.txt.zip') as zin:\n",
        "    with zin.open('jaychou_lyrics.txt') as f:\n",
        "        data = f.read().decode('utf-8')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTJC8tmhNTb7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3cf17f6a-879a-4e9e-9867-92508922f9ee"
      },
      "source": [
        "data = data.replace('\\n', ' ').replace('\\t', ' ')\n",
        "# data = data.lower()\n",
        "data = data[:10000]\n",
        "data_indexs = list(set(data))\n",
        "data_indexs_len = len(data_indexs)\n",
        "idx_to_char = dict([(char, i) for (char, i) in enumerate(data_indexs)])\n",
        "char_to_idx = dict([(i, char) for (char, i) in enumerate(data_indexs)])\n",
        "datas = [char_to_idx[i] for i in data]\n",
        "print(data_indexs_len)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJIDPTzINWDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_iter_consecutive(corpus_indices, batch_size, num_steps, device=None):\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    corpus_indices = torch.tensor(corpus_indices, dtype=torch.float32, device=device)\n",
        "    data_len = len(corpus_indices)\n",
        "    batch_len = data_len // batch_size\n",
        "    indices = corpus_indices[0: batch_size * batch_len].view(batch_size, batch_len)\n",
        "    epoch_size = (batch_len - 1) // num_steps\n",
        "    for i in range(epoch_size):\n",
        "        i = i * num_steps\n",
        "        X = indices[:, i: i + num_steps]\n",
        "        Y = indices[:, i + 1: i + num_steps + 1]\n",
        "        yield X, Y\n",
        "\n",
        "\n",
        "def one_hot(x, n_class, dtype=torch.float32):\n",
        "    # X shape: (batch), output shape: (batch, n_class)\n",
        "    x = x.long()\n",
        "    res = torch.zeros(x.shape[0], n_class, dtype=dtype, device=x.device)\n",
        "    res.scatter_(1, x.view(-1, 1), 1)\n",
        "    return res"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnQca_CbNX5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_onehot(X, n_class):\n",
        "    return [one_hot(X[:, i], n_class) for i in range(X.shape[1])]\n",
        "\n",
        "\n",
        "class MyRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MyRNN, self).__init__()\n",
        "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=1)\n",
        "        self.state = None\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.state = None\n",
        "\n",
        "    def forward(self, x,state):\n",
        "        out, self.state = self.rnn(x,state)\n",
        "        #         self.state = state\n",
        "        # print(out.shape)\n",
        "        # print(out.view(-1,out.shape[-1]).shape)\n",
        "        y = self.fc(out)\n",
        "        return y,self.state"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy1_Tu0eNbTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grad_clipping(params, theta, device):\n",
        "    norm = torch.tensor([0.0], device=device)\n",
        "    for param in params:\n",
        "        norm += (param.grad.data ** 2).sum()\n",
        "    norm = norm.sqrt().item()\n",
        "    if norm > theta:\n",
        "        for param in params:\n",
        "            param.grad.data *= (theta / norm)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUBlQY6lNc2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_rnn_pytorch(prefix, num_chars, model, vocab_size, device, idx_to_char,\n",
        "                        char_to_idx):\n",
        "    state = (torch.zeros((1,1, 256), device=device),\n",
        "            torch.zeros((1,1, 256), device=device))\n",
        "    output = [char_to_idx[prefix[0]]]  # output会记录prefix加上输出\n",
        "    for t in range(num_chars + len(prefix) - 1):\n",
        "        X = torch.tensor([output[-1]], device=device).view(1, 1)\n",
        "        if state is not None:\n",
        "            if isinstance(state, tuple):  # LSTM, state:(h, c)\n",
        "                state = (state[0].to(device), state[1].to(device))\n",
        "            else:\n",
        "                state = state.to(device)\n",
        "        X = to_onehot(X, data_indexs_len)\n",
        "        X = torch.stack(X)\n",
        "        (Y, state) = model(X, state)\n",
        "        if t < len(prefix) - 1:\n",
        "            output.append(char_to_idx[prefix[t + 1]])\n",
        "        else:\n",
        "            output.append(int(Y.argmax().item()))\n",
        "    return ''.join([idx_to_char[i] for i in output])"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQN7X0e2adwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_lstm_state(batch_size, num_hiddens, device):\n",
        "    return (torch.zeros((1,batch_size, num_hiddens), device=device),\n",
        "            torch.zeros((1,batch_size, num_hiddens), device=device))"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTOwbfBWNf1F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "5f80406e-2655-4260-ab13-08506ec022b5"
      },
      "source": [
        "net = MyRNN(1027, 256,1027)\n",
        "num_epochs = 160\n",
        "loss = nn.CrossEntropyLoss()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "net.to(device)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.002)\n",
        "clipping_theta = 1e-2\n",
        "h_state = None\n",
        "h_o = None\n",
        "start = time.time()\n",
        "prefixes = ['分开','不分开']\n",
        "for epoch in range(num_epochs):\n",
        "    state = init_lstm_state(64, 256, device)\n",
        "    sum_l, n = 0, 0\n",
        "    for x, y in data_iter_consecutive(datas, 64, 8):\n",
        "        # print(x.shape)\n",
        "        # break\n",
        "        if state is not None:\n",
        "            if isinstance(state, tuple):\n",
        "                state = (state[0].detach(), state[1].detach())\n",
        "            else:\n",
        "                state = state.detach()\n",
        "        x = to_onehot(x, data_indexs_len)\n",
        "        # print(x)\n",
        "        # print(len(x),x[0].shape)\n",
        "        x = torch.stack(x)\n",
        "        # print(x.shape)\n",
        "        y_hat,state = net(x,state)\n",
        "        # h_state = state.data\n",
        "        # h_o = o.data\n",
        "        # print(y.shape)\n",
        "        y = torch.transpose(y, 0, 1).contiguous().view(-1)\n",
        "        # print(y.shape)\n",
        "        # print(y_hat.shape)\n",
        "        # print(y.shape)\n",
        "        y_hat = y_hat.view(-1, y_hat.shape[-1])\n",
        "        # print(y_hat.shape)\n",
        "        #         print(len(y),y[0].shape)\n",
        "        #         print(len(y_hat),y_hat[0].shape)\n",
        "        #         print(y[0])\n",
        "        #         print(y_hat[0])\n",
        "        #         print(y.shape)\n",
        "        #         print(y_hat.shpae)\n",
        "        l = loss(y_hat, y.long())\n",
        "        sum_l += l.item() * y.shape[0]\n",
        "        n += y.shape[0]\n",
        "        optimizer.zero_grad()\n",
        "        l.backward()\n",
        "        # grad_clipping(net.parameters(), clipping_theta, device)\n",
        "        optimizer.step()\n",
        "        # break\n",
        "    # break\n",
        "    try:\n",
        "        perplexity = math.exp(sum_l / n)\n",
        "    except OverflowError:\n",
        "        perplexity = float('inf')\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "        print('epoch %d, perplexity %f, time %.2f sec' % (\n",
        "            epoch + 1, perplexity, time.time() - start))\n",
        "        for prefix in prefixes:\n",
        "            print(' -', predict_rnn_pytorch(prefix, 50, net, data_indexs_len, device, idx_to_char,char_to_idx))\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 15, perplexity 31.822692, time 2.04 sec\n",
            " - 分开 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我\n",
            " - 不分开 我想要你的让我疯狂的可爱女人 一定在我 我的可爱女人 一定在我 我的可爱女人 一定在我 我的可爱女\n",
            "epoch 30, perplexity 1.848132, time 4.01 sec\n",
            " - 分开始我动 回到当初爱你的时空 停格内容不忠 所有回忆对着我进攻 我的 我不 我不要再想 我不 我不 我\n",
            " - 不分开 我妈妈妈 难道你手不会痛吗 不要再这样打我妈妈 难道你手不会痛吗 不要再这样打我妈妈 难道你手不会\n",
            "epoch 45, perplexity 1.148534, time 5.97 sec\n",
            " - 分开始我动 回到当初爱你的时空 停格内容不忠 所有回忆对着我进攻 我的  三对我 要有一定实我妈妈 难道\n",
            " - 不分开 我妈妈的难熬  说穿了其实我的愿望就怎么小 就怎么每天祈祷我的心跳你知道  杵在伊斯坦堡 却只想你\n",
            "epoch 60, perplexity 1.072375, time 7.93 sec\n",
            " - 分开 我想大声宣布 对你依依不舍 连隔壁邻居都猜到我现在的感受 河边的风 在吹着头发飘动 牵着你的手 一\n",
            " - 不分开 我妈妈的难道 手不会痛吗 其实我回家就 为什么 干什么 日行千里系沙袋 飞檐走壁莫奇怪 去去就来 \n",
            "epoch 75, perplexity 1.037122, time 9.90 sec\n",
            " - 分开 我想大声宣布 对你依依不舍 连隔壁邻居都猜到我现在的感受 河边的风 在吹着头发飘动 牵着你的手 一\n",
            " - 不分开 我爱你的爱写在西元前 深埋在美索不达米亚平原 用楔形文字刻下了永远 那已风化千年的誓言 一切又重演\n",
            "epoch 90, perplexity 1.061999, time 11.88 sec\n",
            " - 分开 我用 你说  不想就这样 化在一起 融化在宇宙里 我每天每天每天在想想想想著你 这样的甜蜜 让我开\n",
            " - 不分开不能够永远单纯没有悲哀 我 想带你骑单车 我 想和你看棒球 想这样没担忧 唱着歌 一直走 我想就这样\n",
            "epoch 105, perplexity 1.025545, time 13.85 sec\n",
            " - 分开 我想大你宣布 对你依依不舍 连隔壁邻居都猜到我现在的感受 河边的风 在吹着头发飘动 牵着你的手 一\n",
            " - 不分开不能够永远单纯没有悲哀 我 想带你骑单车 我 想和你看棒球 想这样没担忧 唱着歌 一直走 我想就这样\n",
            "epoch 120, perplexity 1.039925, time 15.81 sec\n",
            " - 分开 我想大你宣布 对你依依不舍 连隔壁邻居都猜到我现在的感受 河边的风 在吹着头发飘动 牵着你的手 一\n",
            " - 不分开不能够远 那句 一个人 后在我想要你 陪我去吃汉堡  说穿了其实我的愿望就怎么小 就怎么每天祈祷我的\n",
            "epoch 135, perplexity 1.025445, time 17.77 sec\n",
            " - 分开 我一定会呵护著你 也逗你笑 你对我有多重要 我后悔没让你知道 安静的听你撒娇 看你睡著一直到老 就\n",
            " - 不分开不能够永远单纯没有悲哀 我 想带你骑单车 我 想和你看棒球 想这样没担忧 唱着歌 一直走 我想就这样\n",
            "epoch 150, perplexity 1.032545, time 19.73 sec\n",
            " - 分开 我想大声宣布 对你依依不舍 连隔壁邻居都猜到我现在的感受 河边的风 在吹着头发飘动 牵着你的手 一\n",
            " - 不分开不能为爱 我不能 爱情走的太快就像龙卷风 不能承受我已无处可躲 我不要再想 我不要再想 我不 我不 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH5_UONPNhju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC0T5W0EbrzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}